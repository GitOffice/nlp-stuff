{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have run a number of experiments: \n",
    "\n",
    "```bash\n",
    "python score.py --maxevals 50 --save\n",
    "python score.py --maxevals 50 --with_bigrams --save\n",
    "python score.py --packg spacy --maxevals 50 --save\n",
    "python score.py --packg spacy --maxevals 50 --with_bigrams --save\n",
    "\n",
    "python score.py --algo lda --n_topics 20 --with_cv --save\n",
    "python score.py --algo lda --n_topics 50 --with_cv --save\n",
    "python score.py --algo lda --n_topics 20 --with_bigrams --with_cv --save\n",
    "python score.py --algo lda --n_topics 50 --with_bigrams --with_cv --save\n",
    "python score.py --packg spacy --algo lda --n_topics 20 --with_cv --save\n",
    "python score.py --packg spacy --algo lda --n_topics 50 --with_cv --save\n",
    "python score.py --packg spacy --algo lda --n_topics 20 --with_bigrams --with_cv --save\n",
    "python score.py --packg spacy --algo lda --n_topics 50 --with_bigrams --with_cv --save\n",
    "\n",
    "python score.py --algo ensemb --n_topics 20 --with_cv --save\n",
    "python score.py --algo ensemb --n_topics 20 --with_bigrams --with_cv --save\n",
    "python score.py --packg spacy --algo ensemb --n_topics 20 --with_cv --save\n",
    "python score.py --packg spacy --algo ensemb --n_topics 20 --with_bigrams --with_cv --save\n",
    "```\n",
    "\n",
    "I have used LDA, EnStop (pLSA+UMAP) and tfidf with nltk and spacy tokenization (see `preprocessing.py`) with and without bigrams. \n",
    "\n",
    "All the code neccesary to run the experiments can be found in `score.py`. Is mostly this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import argparse\n",
    "import pdb\n",
    "\n",
    "from pathlib import Path\n",
    "# Note: I simply copied the `utils` dir into the notebooks dir so that I can run the next cell here\n",
    "from utils.lightgbm_optimizer import LGBOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "packg = 'nltk'           # nltk or spacy\n",
    "with_bigrams = False\n",
    "algo = 'lda'             # lda or ensemb\n",
    "n_topics = '20'          # 20 or 50 when lda, only 20 for ensemb\n",
    "with_cv = False          # hyperoptimize with cross validation\n",
    "is_unbalance = True      # set the lightgbm is_unbalance param to True/False\n",
    "with_focal_loss = False  # Use the Focal Loss (see here: https://github.com/jrzaurin/LightGBM-with-Focal-Loss)\n",
    "eval_with_metric = False # hyperoptimize using the F1 score or the CE Loss\n",
    "save = False             \n",
    "maxevals = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:04<00:00,  4.02s/it, best loss: 0.9605855850150624]\n",
      "acc: 0.6088, f1 score: 0.5341, precision: 0.5264, recall: 0.6088\n",
      "confusion_matrix\n",
      "[[  652   172   380  1461]\n",
      " [  398   196   532  1916]\n",
      " [  361   185   762  4527]\n",
      " [  284   130   555 15353]]\n"
     ]
    }
   ],
   "source": [
    "FEAT_PATH = Path('../features')\n",
    "\n",
    "wbigram = 'bigram_' if with_bigrams else ''\n",
    "dataname = packg + '_tok_reviews_' + wbigram + algo\n",
    "if algo is not 'tfidf': dataname = dataname + '_' +  n_topics\n",
    "\n",
    "dtrain = pickle.load(open(FEAT_PATH/('train/'+ dataname+'_feat_tr.p'), 'rb'))\n",
    "dvalid = pickle.load(open(FEAT_PATH/('valid/'+ dataname+'_feat_val.p'), 'rb'))\n",
    "dtest  = pickle.load(open(FEAT_PATH/('test/' + dataname+'_feat_te.p'),  'rb'))\n",
    "\n",
    "opt = LGBOptimizer(\n",
    "    dataname,\n",
    "    dtrain,\n",
    "    dvalid,\n",
    "    dtest,\n",
    "    with_cv=with_cv,\n",
    "    is_unbalance=is_unbalance,\n",
    "    with_focal_loss=with_focal_loss,\n",
    "    eval_with_metric=eval_with_metric,\n",
    "    save=save)\n",
    "opt.optimize(maxevals=maxevals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first comment a bit on what happens within `LGBOptimizer`. Here, I use `hyperopt` and the following parameter space: \n",
    "\n",
    "```python\n",
    "space = {\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2),\n",
    "    'num_boost_round': hp.quniform('num_boost_round', 50, 500, 20),\n",
    "    'num_leaves': hp.quniform('num_leaves', 31, 255, 4),\n",
    "    'min_child_weight': hp.uniform('min_child_weight', 0.1, 10),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.),\n",
    "    'subsample': hp.uniform('subsample', 0.5, 1.),\n",
    "    'reg_alpha': hp.uniform('reg_alpha', 0.01, 0.1),\n",
    "    'reg_lambda': hp.uniform('reg_lambda', 0.01, 0.1),\n",
    "}\n",
    "```\n",
    "\n",
    "To optimize LightGBM hyper-parameters. I think I could perhaps refine a bit more some of them, like using a slightly higher `learning_rate` (e.g 0.3) of a smaller `num_boost_round` (e.g. 10). I will leave that to future visits to this repo or to you, the reader ðŸ™‚. \n",
    "\n",
    "Within the class `LGBOptimizer`, one can run the optimization process with a number of options. For example, using cross validation and the F1 score. This will basically run the next piece of code:\n",
    "\n",
    "```python\n",
    "cv_result = lgb.cv(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=params['num_boost_round'],\n",
    "    metrics='multi_logloss',\n",
    "    feval = lgb_f1 if self.eval_with_metric else None,\n",
    "    nfold=3,\n",
    "    stratified=True,\n",
    "    early_stopping_rounds=20)\n",
    "```\n",
    "\n",
    "where lgb_f1 is\n",
    "\n",
    "```python\n",
    "def lgb_f1_score(preds, lgbDataset, num_class):\n",
    "\t\"\"\"\n",
    "\tImplementation of the f1 score to be used as evaluation score for lightgbm\n",
    "\tParameters:\n",
    "\t-----------\n",
    "\tpreds: numpy.ndarray\n",
    "\t\tarray with the predictions\n",
    "\tlgbDataset: lightgbm.Dataset\n",
    "\t\"\"\"\n",
    "\tpreds = preds.reshape(-1, num_class, order='F')\n",
    "\tcat_preds = np.argmax(preds, axis=1)\n",
    "\ty_true = lgbDataset.get_label()\n",
    "\treturn 'f1', f1_score(y_true, cat_preds, average='weighted'), True\n",
    "\n",
    "```\n",
    "\n",
    "If you want to know more about custom losses and metrics for lightgbm, have a look to my repo [here](https://github.com/jrzaurin/LightGBM-with-Focal-Loss). All this can be found in the `utils` module. \n",
    "\n",
    "So, if we choose to run 100 iterations, with cross validation and evaluate using the F1 score, for a set of features that is the results of using LDA with 20 topics, `LGBOptimizer` will take the train, validation and test datasets, merge the first two and run a 3 stratified-fold CV experiments on them. Once the best parameters have been found based on the resulting F1 score, it will then predict on the test set and compute the success metrics. \n",
    "\n",
    "```python\n",
    "acc  = accuracy_score(self.lgtest.label, preds)\n",
    "f1   = f1_score(self.lgtest.label, preds, average='weighted')\n",
    "prec = precision_score(self.lgtest.label, preds, average='weighted')\n",
    "rec  = recall_score(self.lgtest.label, preds, average='weighted')\n",
    "cm   = confusion_matrix(self.lgtest.label, preds)\n",
    "```\n",
    "\n",
    "and \"that's it\". Let's have a look to the results for the 16 experiments at the top of this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_PATH = Path('../results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_fnames = list(RESULT_PATH.glob(\"*.p\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnames = [str(rf).replace('../results/', '') for rf in results_fnames]\n",
    "rnames = [str(rf).replace('_results_unb.p', '') for rf in rnames]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_nlp)",
   "language": "python",
   "name": "conda_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
