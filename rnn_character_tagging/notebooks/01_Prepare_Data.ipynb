{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in this repo is just an old exercise I did a while ago which was in itself a Pytorch adaptation of a post by my good friend [Nadbor](http://nadbordrozd.github.io/blog/2017/06/03/python-or-scala/). I decided to bring to this repo since is NLP related and this is NLP stuff ðŸ™‚. \n",
    "\n",
    "### Get Data\n",
    "\n",
    "First you would need to get the data, for that simply run \n",
    "\n",
    "```bash\n",
    "bash get_data.sh\n",
    "```\n",
    "\n",
    "Then one needs to prepare the input files\n",
    "\n",
    "### Prepare Input files\n",
    "\n",
    "To run this in the terminal you would do\n",
    "\n",
    "```bash\n",
    "python prepare_input_files.py data/austen 'austen.txt' data/austen_clean\n",
    "python prepare_input_files.py data/shakespeare/ 'shakespeare.txt' data/shakespeare_clean\n",
    "python prepare_input_files.py data/scikit-learn '*.py' data/sklearn_clean\n",
    "python prepare_input_files.py data/scalaz/ '*.scala' data/scalaz_clean\n",
    "```\n",
    "\n",
    "`prepare_input_files.py` will call `text_utils.py`. Here in this notebook I include all the code for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import fnmatch\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "from unidecode import unidecode\n",
    "\n",
    "chars = '\\n !\"#$%&\\'()*+,-./0123456789:;<=>?@[\\\\]^_`abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ{|}~'\n",
    "charset = set(chars)\n",
    "n_chars = len(charset)\n",
    "char2ind = dict((c, i) for i, c in enumerate(chars))\n",
    "ind2char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "char2vec = {}\n",
    "for c in charset:\n",
    "    vec = np.zeros(n_chars)\n",
    "    vec[char2ind[c]] = 1\n",
    "    char2vec[c] = vec\n",
    "\n",
    "\n",
    "def sanitize_text(text):\n",
    "    return ''.join(c for c in unidecode(text.decode('utf-8', 'ignore')).replace('\\t', '    ') if c in charset)\n",
    "\n",
    "\n",
    "# '../' because we are in the 'notebooks' dir\n",
    "input_dirs = ['../data/scikit-learn', '../data/scalaz', '../data/austen', '../data/shakespeare']\n",
    "output_dirs = ['../data/sklearn_clean', '../data/scalaz_clean', '../data/austen_clean', '../data/shakespeare_clean']\n",
    "file_patterns = ['*.py','*.scala','austen.txt','shakespeare.txt']\n",
    "for input_dir, output_dir, file_pattern in zip(input_dirs, output_dirs, file_patterns):\n",
    "    try:\n",
    "        os.makedirs(output_dir)\n",
    "    except os.error as e:\n",
    "        # errno 17 means 'file exists error' which we can ignore\n",
    "        if e.errno != 17:\n",
    "            raise\n",
    "\n",
    "    for root, dirnames, filenames in os.walk(input_dir):\n",
    "        for filename in fnmatch.filter(filenames, file_pattern):\n",
    "            src_path = os.path.join(root, filename)\n",
    "            dst_path = os.path.join(output_dir, filename)\n",
    "            # read in bytes (rb), write in text ('w')\n",
    "            with open(src_path, 'rb') as in_f, open(dst_path, 'w') as out_f:\n",
    "                out_f.write(sanitize_text(in_f.read()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most of this excercise we will be using the python and scala datasets.\n",
    "\n",
    "Just in case you want to use Austen's and Shakespeare's books, here the books are splitted so that the partitions \"make sense\", meaning have enough text and correspond to episodes or chapters.\n",
    "\n",
    "If you wanted to run it with the split_ebooks.py script:\n",
    "\n",
    "```bash\n",
    "python split_ebooks.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "authors = ['austen', 'shakespeare']\n",
    "\n",
    "ebook_d = {}\n",
    "ebook_d['austen'] = {}\n",
    "ebook_d['shakespeare'] = {}\n",
    "\n",
    "ebook_d['austen']['dir'] = '../data/austen_clean'\n",
    "ebook_d['austen']['fname'] = 'austen.txt'\n",
    "ebook_d['austen']['regex'] = 'Chapter\\s+.*|CHAPTER\\s+.*' # regular expression to split based on\n",
    "ebook_d['austen']['startidx'] = 1 # starting index for the resulting partitions\n",
    "ebook_d['austen']['endex'] = 'THE END' # expression to denote the end of the document \n",
    "\n",
    "ebook_d['shakespeare']['dir'] = '../data/shakespeare_clean'\n",
    "ebook_d['shakespeare']['fname'] = 'shakespeare.txt'\n",
    "ebook_d['shakespeare']['regex'] = '\\s+\\d+\\s+|ACT\\s+.*\\.|SCENE\\s+.*\\.'\n",
    "ebook_d['shakespeare']['startidx'] = 3\n",
    "ebook_d['shakespeare']['endex'] = 'FINIS'\n",
    "\n",
    "for author in authors:\n",
    "    filepath = os.path.join(ebook_d[author]['dir'],ebook_d[author]['fname'])\n",
    "    with open(filepath, 'r') as f:\n",
    "        ebook = f.read()\n",
    "    f.close()\n",
    "\n",
    "    endex = ebook_d[author]['endex']\n",
    "    startidx = ebook_d[author]['startidx']\n",
    "    the_end = [m.start() for m in re.finditer(endex, ebook)][-1]\n",
    "    ebook = ebook[:the_end]\n",
    "    parts = re.split(ebook_d[author]['regex'], ebook)[startidx:]\n",
    "\n",
    "    for i,p in enumerate(parts):\n",
    "        fname = 'part' + str(i).zfill(4) + '.txt'\n",
    "        fpath = os.path.join(ebook_d[author]['dir'],fname)\n",
    "        with open(fpath, 'w') as f:\n",
    "            f.write(p)\n",
    "        f.close()\n",
    "    os.remove(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test split\n",
    "\n",
    "3-Train/Test split\n",
    "\n",
    "Not much secret here...\n",
    "\n",
    "If you wanted to run it with the .py script:\n",
    "\n",
    "```bash\n",
    "python train_test_split.py data/austen_clean/ 0.25\n",
    "python train_test_split.py data/shakespeare_clean/ 0.25\n",
    "python train_test_split.py data/sklearn_clean/ 0.25\n",
    "python train_test_split.py data/scalaz_clean/ 0.25\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dirs = ['../data/sklearn_clean/', '../data/scalaz_clean/', '../data/austen_clean/', '../data/shakespeare_clean/']\n",
    "test_fraction = 0.25\n",
    "\n",
    "for data_dir in data_dirs:\n",
    "    files = os.listdir(data_dir)\n",
    "    train_dir = os.path.join(data_dir, 'train')\n",
    "    test_dir = os.path.join(data_dir, 'test')\n",
    "\n",
    "    # randomly shuffle the files\n",
    "    files = list(np.array(files)[np.random.permutation(len(files))])\n",
    "    os.makedirs((train_dir))\n",
    "    os.makedirs(test_dir)\n",
    "\n",
    "    train_fraction = 1 - test_fraction\n",
    "    for i, f in enumerate(files):\n",
    "        file_path = os.path.join(data_dir, f)\n",
    "        if len(files) * train_fraction >= i:\n",
    "            shutil.move(file_path, train_dir)\n",
    "        else:\n",
    "            shutil.move(file_path, test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is what you should have in your data dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/austen_clean\n",
      "../data/austen_clean/test\n",
      "../data/austen_clean/train\n",
      "../data/scalaz_clean\n",
      "../data/scalaz_clean/test\n",
      "../data/scalaz_clean/train\n",
      "../data/shakespeare_clean\n",
      "../data/shakespeare_clean/test\n",
      "../data/shakespeare_clean/train\n",
      "../data/sklearn_clean\n",
      "../data/sklearn_clean/test\n",
      "../data/sklearn_clean/train\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "find ../data/*clean -maxdepth 2  -type d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that's it, we have a series of files with text and we are ready to train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
